diff --git a/hadoop-common-project/hadoop-common/CHANGES.txt b/hadoop-common-project/hadoop-common/CHANGES.txt
index 642a88d3be27..c4dcdcac1535 100644
--- a/hadoop-common-project/hadoop-common/CHANGES.txt
+++ b/hadoop-common-project/hadoop-common/CHANGES.txt
@@ -284,7 +284,9 @@ Trunk (Unreleased)
 
     HADOOP-7761. Improve the performance of raw comparisons. (todd)
 
-    HADOOP-8589 ViewFs tests fail when tests and home dirs are nested (sanjay Radia)
+    HADOOP-8589. ViewFs tests fail when tests and home dirs are nested (sanjay Radia)
+
+    HADOOP-9956. RPC listener inefficiently assigns connections to readers (daryn)
 
 Release 2.3.0 - UNRELEASED
 
diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java
index 89f9501d980d..f38f181a8e03 100644
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java
@@ -583,7 +583,8 @@ private synchronized void doRunLoop() {
             readSelector.select();
             while (adding) {
               this.wait(1000);
-            }              
+            }
+            Thread.sleep(200);
 
             Iterator<SelectionKey> iter = readSelector.selectedKeys().iterator();
             while (iter.hasNext()) {
diff --git a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestIPC.java b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestIPC.java
index 33fb799c0c8d..fce6e0d2bdd0 100644
--- a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestIPC.java
+++ b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestIPC.java
@@ -35,21 +35,26 @@
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.OutputStream;
+import java.lang.management.ManagementFactory;
+import java.lang.management.RuntimeMXBean;
+import java.lang.management.ThreadInfo;
+import java.lang.management.ThreadMXBean;
 import java.lang.reflect.Method;
 import java.lang.reflect.Proxy;
 import java.net.InetSocketAddress;
 import java.net.Socket;
 import java.net.SocketTimeoutException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-import java.util.Random;
+import java.util.*;
+import java.util.concurrent.BrokenBarrierException;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.atomic.AtomicInteger;
 
 import javax.net.SocketFactory;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CommonConfigurationKeys;
 import org.apache.hadoop.fs.CommonConfigurationKeysPublic;
 import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.io.IntWritable;
@@ -613,6 +618,300 @@ public void testIpcWithServiceClass() throws IOException {
     server.stop();
   }
 
+
+
+  private static class TestServerQueue extends Server {
+    final CountDownLatch firstCallLatch = new CountDownLatch(1);
+    final CountDownLatch callBlockLatch = new CountDownLatch(1);
+    
+    TestServerQueue(int expectedCalls, int readers, int callQ, int handlers,
+        Configuration conf) throws IOException {
+      super(ADDRESS, 0, LongWritable.class, handlers, readers, callQ, conf, null, null); 
+    }
+
+    @Override
+    public Writable call(RPC.RpcKind rpcKind, String protocol, Writable param,
+        long receiveTime) throws IOException {
+      firstCallLatch.countDown();
+      try {
+        callBlockLatch.await();
+      } catch (InterruptedException e) {
+        throw new IOException(e);
+      }
+      return param;
+    }
+  }
+
+  /**
+   * Check that reader queueing works
+   * @throws BrokenBarrierException 
+   * @throws InterruptedException 
+   */
+  @Test(timeout=60000)
+  public void testIpcWithReaderQueuing() throws Exception {
+    // 1 reader, 1 connectionQ slot, 1 callq
+    for (int i=0; i < 10; i++) {
+      checkBlocking(1, 1, 1);
+    }
+    // 4 readers, 5 connectionQ slots, 2 callq
+    for (int i=0; i < 10; i++) {
+      checkBlocking(4, 5, 2);
+    }
+  }
+
+  @Test(timeout = 600000)
+  public void testLowCPUUtilBySlowReader() throws Exception {
+    int numReaders = 1;
+    int numHandlers = 12;
+    int callQueueSize = 100;
+    int numClients = 120;
+
+    final AtomicInteger failures = new AtomicInteger(0);
+
+    final Configuration conf = new Configuration();
+
+    Server ipcServer = new Server(ADDRESS, 0, LongWritable.class, numHandlers, numReaders, callQueueSize, conf, null, null) {
+      // Heavy job to increase CPU utilization and will finally return the response
+      @Override
+      public Writable call(RPC.RpcKind rpcKind, String protocol,
+                           Writable param, long receiveTime) throws Exception {
+        long durationMillis = 1000;
+        long endTime = System.currentTimeMillis() + durationMillis;
+        long count = 0;
+
+        while (System.currentTimeMillis() < endTime) {
+          count += Math.sqrt(count + 1);
+        }
+
+        return param;
+      }
+    };
+    final InetSocketAddress addr = NetUtils.getConnectAddress(ipcServer);
+    ipcServer.start();
+
+    Client.setConnectTimeout(conf, 20000);
+
+    CpuUtilizationMonitor monitor = new CpuUtilizationMonitor(1000, 0);
+    Thread monitorThread = new Thread(monitor);
+    monitorThread.start();
+
+    // instantiate the threads, will start in batches
+    Thread[] threads = new Thread[numClients];
+    for (int i = 0; i < numClients; i++) {
+      threads[i] = new Thread(new Runnable() {
+        @Override
+        public void run() {
+          Client client = new Client(LongWritable.class, conf);
+          try {
+            client.call(new LongWritable(Thread.currentThread().getId()),
+                    addr, null, null, 600000, conf);
+          } catch (Throwable e) {
+            LOG.error(e);
+            failures.incrementAndGet();
+            return;
+
+          } finally {
+            client.stop();
+          }
+        }
+      });
+    }
+
+    for (int i = 0; i < numClients; i++) {
+      threads[i].start();
+    }
+
+    for (int i = 0; i < numClients; i++) {
+      threads[i].join();
+    }
+
+    monitor.stop();
+    monitorThread.join();
+
+    ipcServer.stop();
+
+    System.out.println("testLowCPUUtilByListenerReader finished");
+
+  }
+
+  public class CpuUtilizationMonitor implements Runnable{
+    private boolean isRunning = false;
+    private int sample_ms; // Sample time in milliseconds
+
+    private int interval_ms; // Loop interval in milliseconds
+    private final ThreadMXBean threadMxBean = ManagementFactory.getThreadMXBean();
+    private final RuntimeMXBean runtimeMxBean = ManagementFactory.getRuntimeMXBean();
+    private final int NR_CPUS = ManagementFactory.getOperatingSystemMXBean().getAvailableProcessors();
+
+    CpuUtilizationMonitor(int sample_ms, int interval_ms) {
+      this.sample_ms = sample_ms;
+      this.interval_ms = interval_ms;
+    }
+
+    public void run() {
+      isRunning = true;
+      int run = 0;
+      while (isRunning) {
+        try {
+          Map<Long, String> cpuUsage = calculateCpuUsageOfThreads();
+          System.out.println("\n" + "Run " + run++ + ": " + cpuUsage + "\n");
+          Thread.sleep(interval_ms); // Wait for the next sample
+        } catch (Exception e) {
+          e.printStackTrace();
+        }
+      }
+    }
+
+    public void stop() {
+      isRunning = false;
+    }
+
+    public Map<Long, String> calculateCpuUsageOfThreads() throws InterruptedException {
+      long initialUptime = runtimeMxBean.getUptime();
+      Map<Long, Long> threadInitialCPU = getThreadCpuTime();
+
+      Thread.sleep(sample_ms);
+
+      long upTime = runtimeMxBean.getUptime();
+      Map<Long, Long> threadCurrentCPU = getThreadCpuTime();
+      long elapsedTime = upTime - initialUptime;
+
+      return calculateCpuUtilization(threadInitialCPU, threadCurrentCPU, elapsedTime);
+    }
+
+    private Map<Long, Long> getThreadCpuTime() {
+      Map<Long, Long> threadCpuTime = new HashMap<Long, Long>();
+      ThreadInfo[] threadInfos = threadMxBean.dumpAllThreads(false, false);
+      for (ThreadInfo info : threadInfos) {
+        threadCpuTime.put(info.getThreadId(), threadMxBean.getThreadCpuTime(info.getThreadId()));
+      }
+      return threadCpuTime;
+    }
+
+    private Map<Long, String> calculateCpuUtilization(Map<Long, Long> initialCpu, Map<Long, Long> currentCpu, long elapsedTime) {
+      Map<Long, String> threadCpuUsage = new HashMap<Long, String>();
+      float sum = 0f;
+      for (ThreadInfo info : threadMxBean.dumpAllThreads(false, false)) {
+        Long initialCPU = initialCpu.get(info.getThreadId());
+        if (initialCPU != null) {
+          long elapsedCpu = currentCpu.get(info.getThreadId()) - initialCPU;
+          float cpuUsage = (elapsedCpu / (elapsedTime * 1000000F * NR_CPUS)) * 100;
+          sum += cpuUsage;
+          String formattedCpuUsage = String.format("%.1f%%", cpuUsage);
+          if (!formattedCpuUsage.equals("0.0%")) {
+            threadCpuUsage.put(info.getThreadId(), formattedCpuUsage);
+          }
+        }
+      }
+      String formattedCpuUsageSum = String.format("%.1f%%", sum);
+      threadCpuUsage.put(-1L, formattedCpuUsageSum);
+
+      return threadCpuUsage;
+    }
+  }
+  
+  // goal is to jam a handler with a connection, fill the callq with
+  // connections, in turn jamming the readers - then flood the server and
+  // ensure that the listener blocks when the reader connection queues fill
+  private void checkBlocking(int readers, int readerQ, int callQ) throws Exception {
+    int handlers = 1; // makes it easier
+    
+    final Configuration conf = new Configuration();
+
+    // send in enough clients to block up the handlers, callq, and readers
+    int initialClients = readers + callQ + handlers;
+    // max connections we should ever end up accepting at once
+    int maxAccept = initialClients + readers*readerQ + 1; // 1 = listener
+    // stress it with 2X the max
+    int clients = maxAccept*2;
+    
+    final AtomicInteger failures = new AtomicInteger(0);
+    final CountDownLatch callFinishedLatch = new CountDownLatch(clients);
+
+    // start server
+    final TestServerQueue server =
+        new TestServerQueue(clients, readers, callQ, handlers, conf);
+    final InetSocketAddress addr = NetUtils.getConnectAddress(server);
+    server.start();
+
+    Client.setConnectTimeout(conf, 10000);
+    
+    // instantiate the threads, will start in batches
+    Thread[] threads = new Thread[clients];
+    for (int i=0; i<clients; i++) {
+      threads[i] = new Thread(new Runnable() {
+        @Override
+        public void run() {
+          Client client = new Client(LongWritable.class, conf);
+          try {
+            client.call(new LongWritable(Thread.currentThread().getId()),
+                addr, null, null, 60000, conf);
+          } catch (Throwable e) {
+            LOG.error(e);
+            failures.incrementAndGet();
+            return;
+          } finally {
+            callFinishedLatch.countDown();            
+            client.stop();
+          }
+        }
+      });
+    }
+    
+    // start enough clients to block up the handler, callq, and each reader;
+    // let the calls sequentially slot in to avoid some readers blocking
+    // and others not blocking in the race to fill the callq
+    for (int i=0; i < initialClients; i++) {
+      threads[i].start();
+      if (i==0) {
+        // let first reader block in a call
+        server.firstCallLatch.await();
+      } else if (i <= callQ) {
+        // let subsequent readers jam the callq, will happen immediately 
+        while (server.getCallQueueLen() != i) {
+          Thread.sleep(1);
+        }
+      } // additional threads block the readers trying to add to the callq
+    }
+
+    // wait till everything is slotted, should happen immediately
+    Thread.sleep(10);
+    if (server.getNumOpenConnections() < initialClients) {
+      LOG.info("(initial clients) need:"+initialClients+" connections have:"+server.getNumOpenConnections());
+      Thread.sleep(100);
+    }
+    LOG.info("ipc layer should be blocked");
+    assertEquals(callQ, server.getCallQueueLen());
+    assertEquals(initialClients, server.getNumOpenConnections());
+    
+    // now flood the server with the rest of the connections, the reader's
+    // connection queues should fill and then the listener should block
+    for (int i=initialClients; i<clients; i++) {
+      threads[i].start();
+    }
+    Thread.sleep(10);
+    if (server.getNumOpenConnections() < maxAccept) {
+      LOG.info("(max clients) need:"+maxAccept+" connections have:"+server.getNumOpenConnections());
+      Thread.sleep(100);
+    }
+    // check a few times to make sure we didn't go over
+    for (int i=0; i<4; i++) {
+      assertEquals(maxAccept, server.getNumOpenConnections());
+      Thread.sleep(100);
+    }
+    
+    // sanity check that no calls have finished
+    assertEquals(clients, callFinishedLatch.getCount());
+    LOG.info("releasing the calls");
+    server.callBlockLatch.countDown();
+    callFinishedLatch.await();
+    for (Thread t : threads) {
+      t.join();
+    }
+    assertEquals(0, failures.get());
+    server.stop();
+  }
+
   /**
    * Make a call from a client and verify if header info is changed in server side
    */
