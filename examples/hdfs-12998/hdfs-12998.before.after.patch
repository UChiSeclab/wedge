diff --git a/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java
index 3883f2f849b7..35b64173a7d5 100644
--- a/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java
+++ b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java
@@ -1994,6 +1994,93 @@ public Void next(final FileSystem fs, final Path p)
     }.resolve(this, absF);
   }
 
+  /**
+   * Returns a remote iterator so that followup calls are made on demand
+   * while consuming the SnapshotDiffReportListing entries.
+   * This reduces memory consumption overhead in case the snapshotDiffReport
+   * is huge.
+   *
+   * @param snapshotDir
+   *          full path of the directory where snapshots are taken
+   * @param fromSnapshot
+   *          snapshot name of the from point. Null indicates the current
+   *          tree
+   * @param toSnapshot
+   *          snapshot name of the to point. Null indicates the current
+   *          tree.
+   * @return Remote iterator
+   */
+  public RemoteIterator
+      <SnapshotDiffReportListing> snapshotDiffReportListingRemoteIterator(
+      final Path snapshotDir, final String fromSnapshot,
+      final String toSnapshot) throws IOException {
+    Path absF = fixRelativePart(snapshotDir);
+    return new FileSystemLinkResolver
+        <RemoteIterator<SnapshotDiffReportListing>>() {
+      @Override
+      public RemoteIterator<SnapshotDiffReportListing> doCall(final Path p)
+          throws IOException {
+        return new SnapshotDiffReportListingIterator(
+            getPathName(p), fromSnapshot, toSnapshot);
+      }
+
+      @Override
+      public RemoteIterator<SnapshotDiffReportListing> next(final FileSystem fs,
+          final Path p) throws IOException {
+        return ((DistributedFileSystem) fs)
+            .snapshotDiffReportListingRemoteIterator(p, fromSnapshot,
+                toSnapshot);
+      }
+    }.resolve(this, absF);
+
+  }
+
+  /**
+   * This class defines an iterator that returns
+   * the SnapshotDiffReportListing for a snapshottable directory
+   * between two given snapshots.
+   */
+  private final class SnapshotDiffReportListingIterator implements
+      RemoteIterator<SnapshotDiffReportListing> {
+    private final String snapshotDir;
+    private final String fromSnapshot;
+    private final String toSnapshot;
+
+    private byte[] startPath;
+    private int index;
+    private boolean hasNext = true;
+
+    private SnapshotDiffReportListingIterator(String snapshotDir,
+        String fromSnapshot, String toSnapshot) {
+      this.snapshotDir = snapshotDir;
+      this.fromSnapshot = fromSnapshot;
+      this.toSnapshot = toSnapshot;
+      this.startPath = DFSUtilClient.EMPTY_BYTES;
+      this.index = -1;
+    }
+
+    @Override
+    public boolean hasNext() {
+      return hasNext;
+    }
+
+    @Override
+    public SnapshotDiffReportListing next() throws IOException {
+      if (!hasNext) {
+        throw new java.util.NoSuchElementException(
+            "No more entry in SnapshotDiffReport for " + snapshotDir);
+      }
+      final SnapshotDiffReportListing part =
+          dfs.getSnapshotDiffReportListing(snapshotDir, fromSnapshot,
+              toSnapshot, startPath, index);
+      startPath = part.getLastPath();
+      index = part.getLastIndex();
+      hasNext =
+          !(Arrays.equals(startPath, DFSUtilClient.EMPTY_BYTES) && index == -1);
+      return part;
+    }
+  }
+
   private SnapshotDiffReport getSnapshotDiffReportInternal(
       final String snapshotDir, final String fromSnapshot,
       final String toSnapshot) throws IOException {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/pom.xml b/hadoop-hdfs-project/hadoop-hdfs/pom.xml
index d6afed1fba54..d99ed88aeec1 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/pom.xml
+++ b/hadoop-hdfs-project/hadoop-hdfs/pom.xml
@@ -220,6 +220,7 @@ http://maven.apache.org/xsd/maven-4.0.0.xsd">
         <groupId>org.apache.maven.plugins</groupId>
         <artifactId>maven-surefire-plugin</artifactId>
         <configuration>
+          <argLine>-Xmx56m -Xms32m</argLine>
           <systemPropertyVariables>
             <startKdc>${startKdc}</startKdc>
             <kdc.resource.dir>${kdc.resource.dir}</kdc.resource.dir>
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java
index a4fb8abd59fb..313a85d45bd3 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java
@@ -23,16 +23,18 @@
 import static org.junit.Assert.fail;
 
 import java.io.IOException;
+import java.lang.management.ManagementFactory;
+import java.lang.management.MemoryMXBean;
+import java.lang.management.MemoryUsage;
 import java.text.SimpleDateFormat;
-import java.util.Date;
-import java.util.EnumSet;
-import java.util.HashMap;
-import java.util.Random;
+import java.util.*;
 
+import org.apache.commons.collections.list.TreeList;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.Options.Rename;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.RemoteIterator;
 import org.apache.hadoop.hdfs.DFSConfigKeys;
 import org.apache.hadoop.hdfs.DFSTestUtil;
 import org.apache.hadoop.hdfs.DFSUtil;
@@ -40,14 +42,20 @@
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.hadoop.hdfs.client.HdfsDataOutputStream;
 import org.apache.hadoop.hdfs.client.HdfsDataOutputStream.SyncFlag;
+import org.apache.hadoop.hdfs.client.impl.SnapshotDiffReportGenerator;
 import org.apache.hadoop.hdfs.protocol.SnapshotDiffReport;
 import org.apache.hadoop.hdfs.protocol.SnapshotDiffReport.DiffReportEntry;
 import org.apache.hadoop.hdfs.protocol.SnapshotDiffReport.DiffType;
+import org.apache.hadoop.hdfs.protocol.SnapshotDiffReportListing;
 import org.apache.hadoop.hdfs.protocol.SnapshotException;
 import org.apache.hadoop.hdfs.server.namenode.INodeDirectory;
 import org.apache.hadoop.hdfs.server.namenode.NameNode;
 import org.apache.hadoop.hdfs.server.namenode.NameNodeAdapter;
+import org.apache.hadoop.metrics2.MetricStringBuilder;
+import org.apache.hadoop.metrics2.MetricsCollector;
+import org.apache.hadoop.metrics2.impl.MetricsCollectorImpl;
 import org.apache.hadoop.test.GenericTestUtils;
+import org.apache.hadoop.util.ChunkedArrayList;
 import org.apache.hadoop.util.Time;
 import org.junit.After;
 import org.junit.Assert;
@@ -1409,4 +1417,308 @@ public void testDiffReportWithRpcLimit2() throws Exception {
         new DiffReportEntry(DiffType.DELETE,
             DFSUtil.string2Bytes("dir3/file3")));
   }
+
+  private void verifyDiffReportForGivenReport(Path dirPath, String from,
+      String to, SnapshotDiffReport report, DiffReportEntry... entries)
+      throws IOException {
+    // reverse the order of from and to
+    SnapshotDiffReport inverseReport =
+        hdfs.getSnapshotDiffReport(dirPath, to, from);
+    LOG.info(report.toString());
+    LOG.info(inverseReport.toString() + "\n");
+
+    assertEquals(entries.length, report.getDiffList().size());
+    assertEquals(entries.length, inverseReport.getDiffList().size());
+
+    for (DiffReportEntry entry : entries) {
+      if (entry.getType() == DiffType.MODIFY) {
+        assertTrue(report.getDiffList().contains(entry));
+        assertTrue(inverseReport.getDiffList().contains(entry));
+      } else if (entry.getType() == DiffType.DELETE) {
+        assertTrue(report.getDiffList().contains(entry));
+        assertTrue(inverseReport.getDiffList().contains(
+            new DiffReportEntry(DiffType.CREATE, entry.getSourcePath())));
+      } else if (entry.getType() == DiffType.CREATE) {
+        assertTrue(report.getDiffList().contains(entry));
+        assertTrue(inverseReport.getDiffList().contains(
+            new DiffReportEntry(DiffType.DELETE, entry.getSourcePath())));
+      }
+    }
+  }
+
+  @Test
+  public void testSnapshotDiffReportRemoteIterator() throws Exception {
+    final Path root = new Path("/");
+    hdfs.mkdirs(root);
+    for (int i = 1; i <= 3; i++) {
+      final Path path = new Path(root, "dir" + i);
+      hdfs.mkdirs(path);
+    }
+    for (int i = 1; i <= 3; i++) {
+      final Path path = new Path(root, "dir" + i);
+      for (int j = 1; j < 4; j++) {
+        final Path file = new Path(path, "file" + j);
+        DFSTestUtil.createFile(hdfs, file, BLOCKSIZE, REPLICATION, SEED);
+      }
+    }
+    SnapshotTestHelper.createSnapshot(hdfs, root, "s0");
+    Path targetDir = new Path(root, "dir4");
+    //create directory dir4
+    hdfs.mkdirs(targetDir);
+    //moves files from dir1 to dir4
+    Path path = new Path(root, "dir1");
+    for (int j = 1; j < 4; j++) {
+      final Path srcPath = new Path(path, "file" + j);
+      final Path targetPath = new Path(targetDir, "file" + j);
+      hdfs.rename(srcPath, targetPath);
+    }
+    targetDir = new Path(root, "dir3");
+    //overwrite existing files in dir3 from files in dir1
+    path = new Path(root, "dir2");
+    for (int j = 1; j < 4; j++) {
+      final Path srcPath = new Path(path, "file" + j);
+      final Path targetPath = new Path(targetDir, "file" + j);
+      hdfs.rename(srcPath, targetPath, Rename.OVERWRITE);
+    }
+    final Path pathToRename = new Path(root, "dir2");
+    //move dir2 inside dir3
+    hdfs.rename(pathToRename, targetDir);
+    SnapshotTestHelper.createSnapshot(hdfs, root, "s1");
+    RemoteIterator<SnapshotDiffReportListing> iterator =
+        hdfs.snapshotDiffReportListingRemoteIterator(root, "s0", "s1");
+    SnapshotDiffReportGenerator snapshotDiffReport;
+    List<SnapshotDiffReportListing.DiffReportListingEntry> modifiedList =
+        new TreeList();
+    List<SnapshotDiffReportListing.DiffReportListingEntry> createdList =
+        new ChunkedArrayList<>();
+    List<SnapshotDiffReportListing.DiffReportListingEntry> deletedList =
+        new ChunkedArrayList<>();
+    SnapshotDiffReportListing report = null;
+    List<SnapshotDiffReportListing> reportList = new ArrayList<>();
+    while (iterator.hasNext()) {
+      report = iterator.next();
+      reportList.add(report);
+      modifiedList.addAll(report.getModifyList());
+      createdList.addAll(report.getCreateList());
+      deletedList.addAll(report.getDeleteList());
+    }
+    try {
+      iterator.next();
+    } catch (Exception e) {
+      Assert.assertTrue(
+          e.getMessage().contains("No more entry in SnapshotDiffReport for /"));
+    }
+    Assert.assertNotEquals(0, reportList.size());
+    // generate the snapshotDiffReport and Verify
+    snapshotDiffReport = new SnapshotDiffReportGenerator("/", "s0", "s1",
+        report.getIsFromEarlier(), modifiedList, createdList, deletedList);
+    verifyDiffReportForGivenReport(root, "s0", "s1",
+        snapshotDiffReport.generateReport(),
+        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes("")),
+        new DiffReportEntry(DiffType.CREATE, DFSUtil.string2Bytes("dir4")),
+        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes("dir2"),
+            DFSUtil.string2Bytes("dir3/dir2")),
+        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes("dir1")),
+        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes("dir1/file1"),
+            DFSUtil.string2Bytes("dir4/file1")),
+        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes("dir1/file2"),
+            DFSUtil.string2Bytes("dir4/file2")),
+        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes("dir1/file3"),
+            DFSUtil.string2Bytes("dir4/file3")),
+        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes("dir2")),
+        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes("dir2/file1"),
+            DFSUtil.string2Bytes("dir3/file1")),
+        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes("dir2/file2"),
+            DFSUtil.string2Bytes("dir3/file2")),
+        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes("dir2/file3"),
+            DFSUtil.string2Bytes("dir3/file3")),
+        new DiffReportEntry(DiffType.MODIFY, DFSUtil.string2Bytes("dir3")),
+        new DiffReportEntry(DiffType.DELETE,
+            DFSUtil.string2Bytes("dir3/file1")),
+        new DiffReportEntry(DiffType.DELETE,
+            DFSUtil.string2Bytes("dir3/file1")),
+        new DiffReportEntry(DiffType.DELETE,
+            DFSUtil.string2Bytes("dir3/file3")));
+  }
+
+
+  public void setUpSnapshotDiffReportTooBig(Path root, long numFiles) throws Exception {
+    // Create many files
+    hdfs.mkdirs(root);
+    hdfs.mkdirs(new Path(root, "dir1"));
+    hdfs.mkdirs(new Path(root, "dir2"));
+
+    final Path dir1Path = new Path(root, "dir1");
+    for (int j = 0; j < numFiles; j++) {
+      final Path file = new Path(dir1Path, "file" + j);
+      DFSTestUtil.createFile(hdfs, file, 0, (short) 1, SEED);
+    }
+    SnapshotTestHelper.createSnapshot(hdfs, root, "s1");
+
+    // Modify the contents of the directory
+    Path dir2Path = new Path(root, "dir2");
+    for (int j = 1; j < numFiles; j++) {
+      final Path srcPath = new Path(dir1Path, "file" + j);
+      final Path targetPath = new Path(dir2Path, "file" + j);
+      hdfs.rename(srcPath, targetPath);
+    }
+    SnapshotTestHelper.createSnapshot(hdfs, root, "s2");
+  }
+
+  @Test
+  public void testupSnapshotDiffReportTooBigOld() throws Exception {
+    int stride = 100;
+    long numFiles = 20000;
+    String rootPath = "/";
+    final Path root = new Path(rootPath);
+
+    setUpSnapshotDiffReportTooBig(root, numFiles);
+
+    System.out.println("Finished FS operations and snapshot generation. Now start gettting the snapshot diff");
+    long memoryMonitorInterval = 5;
+    MemoryMonitor monitor = new jvmMetricsMemoryMonitor(memoryMonitorInterval);
+    Thread monitorThread = new Thread(monitor);
+    monitorThread.start();
+
+    SnapshotDiffReport report;
+
+    // The old way (buggy version)
+    report = hdfs.getSnapshotDiffReport(root, "s1", "s2");
+
+    // We only care about a few diffs
+    List<DiffReportEntry> diffEntries = report.getDiffList();
+    System.out.println("Different entries size=" + diffEntries.size());
+//    List<DiffReportEntry> selectedDiffEntries = new LinkedList<>();
+//    for (int index = 0; index < diffEntries.size(); index += stride) {
+//      selectedDiffEntries.add(diffEntries.get(index));
+//    }
+//
+//    System.out.println("selectedDiffEntries size=" + selectedDiffEntries.size());
+
+    monitor.stop();
+    monitorThread.join();
+
+    System.out.println("testupSnapshotDiffReportTooBigOld finished");
+  }
+
+  @Test
+  public void testupSnapshotDiffReportTooBigNew() throws Exception {
+    int stride = 100;
+    long numFiles = 20000;
+    String rootPath = "/";
+    final Path root = new Path(rootPath);
+    setUpSnapshotDiffReportTooBig(root, numFiles);
+
+    System.out.println("Finished FS operations and snapshot generation. Now start gettting the snapshot diff");
+    long memoryMonitorInterval = 1;
+    MemoryMonitor monitor = new jvmMetricsMemoryMonitor(memoryMonitorInterval);
+    Thread monitorThread = new Thread(monitor);
+    monitorThread.start();
+
+//    SnapshotDiffReport report;
+//    // The patched way
+    RemoteIterator<SnapshotDiffReportListing> iterator =
+            hdfs.snapshotDiffReportListingRemoteIterator(root, "s1", "s2");
+//    List<SnapshotDiffReportListing.DiffReportListingEntry> modifiedList = new TreeList();
+//    List<SnapshotDiffReportListing.DiffReportListingEntry> createdList = new ChunkedArrayList<>();
+//    List<SnapshotDiffReportListing.DiffReportListingEntry> deletedList = new ChunkedArrayList<>();
+    SnapshotDiffReportListing reportListing = null;
+
+    while (iterator.hasNext()) {
+      reportListing = iterator.next();
+//      modifiedList.addAll(reportListing.getModifyList());
+//      createdList.addAll(reportListing.getCreateList());
+//      deletedList.addAll(reportListing.getDeleteList());
+//
+//      int curSteps = 0;
+//      while (iterator.hasNext() && ++curSteps < stride) {
+//        iterator.next();
+//      }
+    }
+
+//    SnapshotDiffReportGenerator snapshotDiffReportGenerator = new SnapshotDiffReportGenerator(rootPath, "s1", "s2",
+//            reportListing.getIsFromEarlier(), modifiedList, createdList, deletedList);
+//    report = snapshotDiffReportGenerator.generateReport();
+//
+//    List<DiffReportEntry> selectedDiffEntries = report.getDiffList();
+//
+//    System.out.println("selectedDiffEntries size=" + selectedDiffEntries.size());
+
+    monitor.stop();
+    monitorThread.join();
+
+    System.out.println("testupSnapshotDiffReportTooBigNew finished");
+  }
+
+  abstract class MemoryMonitor implements Runnable {
+    protected boolean isRunning;
+    protected long interval;
+
+    public MemoryMonitor (long interval) {
+      this.interval = interval;
+      isRunning = false;
+    }
+
+    @Override
+    public void run() {
+      isRunning = true;
+
+      try {
+        while (isRunning) {
+          printMemoryUsage();
+          Thread.sleep(interval);
+        }
+      } catch (InterruptedException e) {
+        e.printStackTrace();
+      }
+    }
+
+    public void stop() {
+      isRunning = false;
+    }
+
+    abstract public void printMemoryUsage();
+  }
+
+  class systemMemoryMonitor extends MemoryMonitor {
+    Runtime runtime;
+    long maxMemory;
+    long totalMemory;
+
+    public systemMemoryMonitor(long interval) {
+      super(interval);
+      runtime = Runtime.getRuntime();
+      maxMemory = runtime.maxMemory(); // Max memory the JVM can use
+      totalMemory = runtime.totalMemory(); // Total memory currently available to the JVM
+      System.out.println("Max memory the JVM will attempt to use (bytes): " + maxMemory);
+      System.out.println("Total memory available to JVM (bytes): " + totalMemory);
+    }
+
+    @Override
+    public void printMemoryUsage() {
+      long usedMemory = totalMemory - runtime.freeMemory();
+      System.out.println("Used memory (MB): " + usedMemory / 1000000);
+    }
+  }
+
+
+  class jvmMetricsMemoryMonitor extends MemoryMonitor {
+    MemoryMXBean memoryMXBean;
+
+    public jvmMetricsMemoryMonitor(long interval) {
+      super(interval);
+      memoryMXBean = ManagementFactory.getMemoryMXBean();
+      MemoryUsage heapMemoryUsage = memoryMXBean.getHeapMemoryUsage();
+      MemoryUsage nonHeapMemoryUsage = memoryMXBean.getNonHeapMemoryUsage();
+      System.out.println("Heap memory (MB): " + heapMemoryUsage + "\tNon-heap memory (MB): " + nonHeapMemoryUsage);
+    }
+
+    @Override
+    public void printMemoryUsage() {
+      MemoryUsage heapMemoryUsage = memoryMXBean.getHeapMemoryUsage();
+      MemoryUsage nonHeapMemoryUsage = memoryMXBean.getNonHeapMemoryUsage();
+      System.out.println("Used heap memory (MB): " + memoryMXBean.getHeapMemoryUsage().getUsed() / (1024 * 1024) +
+              "\tUsed non-heap memory (MB): " + memoryMXBean.getNonHeapMemoryUsage().getUsed() / (1024 * 1024));
+    }
+  }
 }
diff --git a/hadoop-project/pom.xml b/hadoop-project/pom.xml
index ce51c994a39f..21cb2a93618a 100644
--- a/hadoop-project/pom.xml
+++ b/hadoop-project/pom.xml
@@ -1601,7 +1601,7 @@
         <artifactId>maven-surefire-plugin</artifactId>
         <configuration>
           <reuseForks>false</reuseForks>
-          <forkedProcessTimeoutInSeconds>${surefire.fork.timeout}</forkedProcessTimeoutInSeconds>
+<!--          <forkedProcessTimeoutInSeconds>${surefire.fork.timeout}</forkedProcessTimeoutInSeconds>-->
           <argLine>${maven-surefire-plugin.argLine}</argLine>
           <environmentVariables>
             <HADOOP_COMMON_HOME>${hadoop.common.build.dir}</HADOOP_COMMON_HOME>
