diff --git a/hadoop-mapreduce-project/CHANGES.txt b/hadoop-mapreduce-project/CHANGES.txt
index 5a37ce809c84..0632f7a0f1af 100644
--- a/hadoop-mapreduce-project/CHANGES.txt
+++ b/hadoop-mapreduce-project/CHANGES.txt
@@ -19,6 +19,9 @@ Release 2.9.0 - UNRELEASED
     MAPREDUCE-6640. mapred job -history command should be able to take
     Job ID (rkanter)
 
+    MAPREDUCE-6622. Add capability to set JHS job cache to a
+    task-based limit (rchiang via rkanter)
+
   OPTIMIZATIONS
 
   BUG FIXES
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistory.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistory.java
index de0de7dfdacb..2337f604f760 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistory.java
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistory.java
@@ -18,6 +18,9 @@
 
 package org.apache.hadoop.mapreduce.v2.hs;
 
+import java.lang.management.ManagementFactory;
+import java.lang.management.MemoryMXBean;
+import java.lang.management.MemoryUsage;
 import java.util.Map;
 
 import java.io.IOException;
@@ -28,86 +31,339 @@
 import org.apache.hadoop.fs.FileContext;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapreduce.v2.api.records.JobId;
-import org.apache.hadoop.mapreduce.v2.api.records.JobState;
+import org.apache.hadoop.mapred.JobACLsManager;
+import org.apache.hadoop.mapred.TaskCompletionEvent;
+import org.apache.hadoop.mapreduce.Counters;
+import org.apache.hadoop.mapreduce.JobACL;
+import org.apache.hadoop.mapreduce.v2.api.records.*;
+import org.apache.hadoop.mapreduce.v2.app.job.Task;
 import org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.HistoryFileInfo;
 import org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.JobListCache;
 import org.apache.hadoop.mapreduce.v2.hs.webapp.dao.JobsInfo;
 import org.apache.hadoop.mapreduce.v2.jobhistory.JHAdminConfig;
 import org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.security.authorize.AccessControlList;
+import org.apache.hadoop.yarn.api.records.Priority;
+import org.apache.hadoop.yarn.exceptions.YarnRuntimeException;
 import org.junit.After;
 import org.junit.Test;
 import org.mockito.Mockito;
 
-import static org.junit.Assert.assertEquals;
-import static org.mockito.Mockito.*;
-import org.apache.hadoop.mapreduce.v2.app.job.Job;
-
+import static junit.framework.TestCase.assertEquals;
 import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNull;
 import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+import static org.mockito.Matchers.any;
+import static org.mockito.Matchers.eq;
+import static org.mockito.Mockito.doNothing;
+import static org.mockito.Mockito.doReturn;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.spy;
+import static org.mockito.Mockito.timeout;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.when;
+import org.mockito.stubbing.Answer;
+
+import org.apache.hadoop.mapreduce.v2.app.job.Job;
+import org.mockito.invocation.InvocationOnMock;
+import org.mockito.stubbing.OngoingStubbing;
 
 public class TestJobHistory {
 
   JobHistory jobHistory = null;
 
+  // To mimic a big job
+  class LargeJob implements Job {
+    // The size of a Job (the max size of a task is 12000 according to Jira).
+    // So here we assume it can be as big as to have 5k tasks (according to Jira it can have up to 250k which is too large for a job)
+    private byte[] largeMemoryBlock = new byte[1024 * 12 * 1024 * 5];
+
+
+    public LargeJob() {
+      System.out.println("LargeJob constructed");
+    }
+
+    @Override
+    protected void finalize() throws Throwable {
+      System.out.println("LargeJob deconstructed");
+      super.finalize();
+    }
+
+    @Override
+    public JobId getID() {
+      return null;
+    }
+
+    @Override
+    public String getName() {
+      return null;
+    }
+
+    @Override
+    public JobState getState() {
+      return null;
+    }
+
+    @Override
+    public JobReport getReport() {
+      return null;
+    }
+
+    @Override
+    public Counters getAllCounters() {
+      return null;
+    }
+
+    @Override
+    public Map<TaskId, Task> getTasks() {
+      return null;
+    }
+
+    @Override
+    public Map<TaskId, Task> getTasks(TaskType taskType) {
+      return null;
+    }
+
+    @Override
+    public Task getTask(TaskId taskID) {
+      return null;
+    }
+
+    @Override
+    public List<String> getDiagnostics() {
+      return null;
+    }
+
+    @Override
+    public int getTotalMaps() {
+      return 0;
+    }
+
+    @Override
+    public int getTotalReduces() {
+      return 0;
+    }
+
+    @Override
+    public int getCompletedMaps() {
+      return 0;
+    }
+
+    @Override
+    public int getCompletedReduces() {
+      return 0;
+    }
+
+    @Override
+    public float getProgress() {
+      return 0;
+    }
+
+    @Override
+    public boolean isUber() {
+      return false;
+    }
+
+    @Override
+    public String getUserName() {
+      return null;
+    }
+
+    @Override
+    public String getQueueName() {
+      return null;
+    }
+
+    @Override
+    public Path getConfFile() {
+      return null;
+    }
+
+    @Override
+    public Configuration loadConfFile() throws IOException {
+      return null;
+    }
+
+    @Override
+    public Map<JobACL, AccessControlList> getJobACLs() {
+      return null;
+    }
+
+    @Override
+    public TaskAttemptCompletionEvent[] getTaskAttemptCompletionEvents(int fromEventId, int maxEvents) {
+      return new TaskAttemptCompletionEvent[0];
+    }
+
+    @Override
+    public TaskCompletionEvent[] getMapAttemptCompletionEvents(int startIndex, int maxEvents) {
+      return new TaskCompletionEvent[0];
+    }
+
+    @Override
+    public List<AMInfo> getAMInfos() {
+      return null;
+    }
+
+    @Override
+    public boolean checkAccess(UserGroupInformation callerUGI, JobACL jobOperation) {
+      return false;
+    }
+
+    @Override
+    public void setQueueName(String queueName) {
+
+    }
+
+    @Override
+    public void setJobPriority(Priority priority) {
+
+    }
+  }
+
+  // Since the buggy and patched versions are using two different datastructures for cache, we need to separate the test
   @Test
-  public void testRefreshLoadedJobCache() throws Exception {
+  public void testTasksCacheReal() throws Exception {
     HistoryFileManager historyManager = mock(HistoryFileManager.class);
     jobHistory = spy(new JobHistory());
-    doReturn(historyManager).when(jobHistory).createHistoryFileManager();
+    doReturn(historyManager).when(jobHistory)
+            .createHistoryFileManager();
 
     Configuration conf = new Configuration();
-    // Set the cache size to 2
-    conf.set(JHAdminConfig.MR_HISTORY_LOADED_JOB_CACHE_SIZE, "2");
+    // Set the cache threshold to 50 tasks
+    conf.setInt(JHAdminConfig.MR_HISTORY_LOADED_JOB_CACHE_SIZE, 50);
     jobHistory.init(conf);
     jobHistory.start();
 
     CachedHistoryStorage storage = spy((CachedHistoryStorage) jobHistory
-        .getHistoryStorage());
+            .getHistoryStorage());
 
-    Job[] jobs = new Job[3];
-    JobId[] jobIds = new JobId[3];
+//    assertEquals(storage.getLoadedJobCache().size(), 50);
 
-    for (int i = 0; i < 3; i++) {
-      jobs[i] = mock(Job.class);
+    // Create a bunch of smaller jobs (<< 50 tasks)
+    // We don't save Job here, otherwise GC won't reclaim the evicted Jobs
+//    final Job[] jobs = new Job[50];
+    final JobId[] jobIds = new JobId[50];
+    for (int i = 0; i < jobIds.length; i++) {
       jobIds[i] = mock(JobId.class);
-      when(jobs[i].getID()).thenReturn(jobIds[i]);
     }
 
     HistoryFileInfo fileInfo = mock(HistoryFileInfo.class);
     when(historyManager.getFileInfo(any(JobId.class))).thenReturn(fileInfo);
-    when(fileInfo.loadJob()).thenReturn(jobs[0]).thenReturn(jobs[1])
-        .thenReturn(jobs[2]);
+
+    Answer<Job> answer = new Answer<Job>() {
+      private int count = 0;
+
+      @Override
+      public Job answer(InvocationOnMock invocation) throws Throwable {
+        System.out.println("answer has been called, count=" + count);
+        // We can't use mock here because Mockito mock doesn't allocate memory for any variables
+//        jobs[count] = mock(LargeJob.class);
+        Job job = spy(new LargeJob());
+        when(job.getID()).thenReturn(jobIds[count]);
+        when(job.getTotalMaps()).thenReturn(25);
+        when(job.getTotalReduces()).thenReturn(25);
+        count = Math.min(count + 1, jobIds.length - 1); // Prevent ArrayIndexOutOfBoundsException
+        return job;
+      }
+
+    };
+    when(fileInfo.loadJob()).thenAnswer(answer);
+
+    // The logs have proven that answer() (the function) is called not when you register the callback, but when the monitored function is invoked
+    System.out.println("Before starting monitor");
+
+    long monitorInterval = 5;
+    MemoryMonitor monitor = new jvmMetricsMemoryMonitor(monitorInterval);
+    Thread monitorThread = new Thread(monitor);
+    monitorThread.start();
+
+    Thread.sleep(monitorInterval * 10);
 
     // getFullJob will put the job in the cache if it isn't there
-    for (int i = 0; i < 3; i++) {
-      storage.getFullJob(jobs[i].getID());
+    Map<JobId, Job> jobCache = storage.getLoadedJobCache();
+    for (int i = 0; i < jobIds.length; i++) {
+      storage.getFullJob(jobIds[i]);
+      System.out.println("jobCache size=" + jobCache.size());
     }
 
-    Map<JobId, Job> jobCache = storage.getLoadedJobCache();
-    // job0 should have been purged since cache size is 2
-    assertFalse(jobCache.containsKey(jobs[0].getID()));
-    assertTrue(jobCache.containsKey(jobs[1].getID())
-        && jobCache.containsKey(jobs[2].getID()));
+    Thread.sleep(monitorInterval * 10);
+    monitor.stop();
+    monitorThread.join();
 
-    // Setting cache size to 3
-    conf.set(JHAdminConfig.MR_HISTORY_LOADED_JOB_CACHE_SIZE, "3");
-    doReturn(conf).when(storage).createConf();
+    System.out.println("testTasksCacheReal finished");
+  }
 
-    when(fileInfo.loadJob()).thenReturn(jobs[0]).thenReturn(jobs[1])
-        .thenReturn(jobs[2]);
+  abstract class MemoryMonitor implements Runnable {
+    protected boolean isRunning;
+    protected long interval;
 
-    jobHistory.refreshLoadedJobCache();
+    public MemoryMonitor (long interval) {
+      this.interval = interval;
+      isRunning = false;
+    }
 
-    for (int i = 0; i < 3; i++) {
-      storage.getFullJob(jobs[i].getID());
+    @Override
+    public void run() {
+      isRunning = true;
+
+      try {
+        while (isRunning) {
+          printMemoryUsage();
+          Thread.sleep(interval);
+        }
+      } catch (InterruptedException e) {
+        e.printStackTrace();
+      }
     }
 
-    jobCache = storage.getLoadedJobCache();
+    public void stop() {
+      isRunning = false;
+    }
+
+    abstract public void printMemoryUsage();
+  }
+
+  class systemMemoryMonitor extends MemoryMonitor {
+    Runtime runtime;
+    long maxMemory;
+    long totalMemory;
+
+    public systemMemoryMonitor(long interval) {
+      super(interval);
+      runtime = Runtime.getRuntime();
+      maxMemory = runtime.maxMemory(); // Max memory the JVM can use
+      totalMemory = runtime.totalMemory(); // Total memory currently available to the JVM
+      System.out.println("Max memory the JVM will attempt to use (bytes): " + maxMemory);
+      System.out.println("Total memory available to JVM (bytes): " + totalMemory);
+    }
+
+    @Override
+    public void printMemoryUsage() {
+      long usedMemory = totalMemory - runtime.freeMemory();
+      System.out.println("Used memory (MB): " + usedMemory / 1000000);
+    }
+  }
+
+
+  class jvmMetricsMemoryMonitor extends MemoryMonitor {
+    MemoryMXBean memoryMXBean;
+
+    public jvmMetricsMemoryMonitor(long interval) {
+      super(interval);
+      memoryMXBean = ManagementFactory.getMemoryMXBean();
+      MemoryUsage heapMemoryUsage = memoryMXBean.getHeapMemoryUsage();
+      MemoryUsage nonHeapMemoryUsage = memoryMXBean.getNonHeapMemoryUsage();
+      System.out.println("Heap memory (MB): " + heapMemoryUsage + "\tNon-heap memory (MB): " + nonHeapMemoryUsage);
+    }
 
-    // All three jobs should be in cache since its size is now 3
-    for (int i = 0; i < 3; i++) {
-      assertTrue(jobCache.containsKey(jobs[i].getID()));
+    @Override
+    public void printMemoryUsage() {
+      MemoryUsage heapMemoryUsage = memoryMXBean.getHeapMemoryUsage();
+      MemoryUsage nonHeapMemoryUsage = memoryMXBean.getNonHeapMemoryUsage();
+      System.out.println("Used heap memory (MB): " + memoryMXBean.getHeapMemoryUsage().getUsed() / (1024 * 1024) +
+              "\tUsed non-heap memory (MB): " + memoryMXBean.getNonHeapMemoryUsage().getUsed() / (1024 * 1024));
     }
   }
 
