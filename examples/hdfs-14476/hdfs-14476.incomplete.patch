diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java
index 445e0212043c..aa5cdb8fee2f 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java
@@ -66,7 +66,7 @@
       LoggerFactory.getLogger(DirectoryScanner.class);
 
   private static final int DEFAULT_MAP_SIZE = 32768;
-
+  private static final int RECONCILE_BLOCKS_BATCH_SIZE = 1000;
   private final FsDatasetSpi<?> dataset;
   private final ExecutorService reportCompileThreadPool;
   private final ScheduledExecutorService masterThread;
@@ -424,10 +424,23 @@ void shutdown() {
    */
   @VisibleForTesting
   public void reconcile() throws IOException {
+    LOG.debug("reconcile start DirectoryScanning");
     scan();
 
+    // HDFS-14476: run checkAndUpadte with batch to avoid holding the lock too
+    // long
+    int loopCount = 0;
     for (final Map.Entry<String, ScanInfo> entry : diffs.getEntries()) {
       dataset.checkAndUpdate(entry.getKey(), entry.getValue());
+
+      if (loopCount % RECONCILE_BLOCKS_BATCH_SIZE == 0) {
+        try {
+          Thread.sleep(2000);
+        } catch (InterruptedException e) {
+          // do nothing
+        }
+      }
+      loopCount++;
     }
 
     if (!retainDiffs) {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDirectoryScanner.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDirectoryScanner.java
index 25470018a962..7274674e3ada 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDirectoryScanner.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDirectoryScanner.java
@@ -361,6 +361,7 @@ private void verifyStats(long totalBlocks, int diffsize, long missingMetaFile,
     assertEquals(duplicateBlocks, stats.duplicateBlocks);
   }
 
+  // Frank: This one will have mismatched block (1)
   @Test(timeout = 300000)
   public void testRetainBlockOnPersistentStorage() throws Exception {
     cluster = new MiniDFSCluster.Builder(CONF)
@@ -464,6 +465,7 @@ public void testScanDirectoryStructureWarn() throws Exception {
     }
   }
 
+  // Frank: This one will have mismatched block (1)
   @Test(timeout = 300000)
   public void testDeleteBlockOnTransientStorage() throws Exception {
     cluster = new MiniDFSCluster.Builder(CONF)
@@ -505,6 +507,7 @@ public void testDeleteBlockOnTransientStorage() throws Exception {
     }
   }
 
+  // Frank: This one will have mismatched block (1)
   @Test(timeout = 600000)
   public void testDirectoryScanner() throws Exception {
     // Run the test with and without parallel scanning
@@ -646,6 +649,80 @@ public void runTest(int parallelism) throws Exception {
     }
   }
 
+  @Test
+  public void testDirectoryScannerBlockingDataNode() throws Exception {
+    int parallelism = 16;
+    int initNumBlockFiles = 10;
+    int addedNumBlockFiles = 100000;
+    int numWriteOps = 25000;
+    CONF.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, "/proj/osu-nfs-test-PG0/tianxi/tmp/hadoop/");
+    CONF.setInt(DFSConfigKeys.DFS_DATANODE_DIRECTORYSCAN_THREADS_KEY,
+            parallelism);
+    cluster = new MiniDFSCluster.Builder(CONF).build();
+    try {
+      cluster.waitActive();
+      bpid = cluster.getNamesystem().getBlockPoolId();
+      fds = DataNodeTestUtils.getFSDataset(cluster.getDataNodes().get(0));
+      client = cluster.getFileSystem().getClient();
+
+
+      scanner = new DirectoryScanner(fds, CONF);
+      scanner.setRetainDiffs(true);
+
+      // Add files with 100 blocks
+      createFile(GenericTestUtils.getMethodName(), BLOCK_LENGTH * initNumBlockFiles, false);
+      long totalBlocks = initNumBlockFiles;
+
+      long blockId;
+      long startWriteTime = System.currentTimeMillis();
+      for (int i = 0; i < addedNumBlockFiles; i++) {
+        createBlockFile();
+      }
+      long endWriteTime = System.currentTimeMillis();
+      double preWriteDurationS = (double) (endWriteTime - startWriteTime) / 1000;
+      System.out.println("Pre-write Ops=" + addedNumBlockFiles
+              + ", time(s)=" + preWriteDurationS
+              + ", throughput=" + (addedNumBlockFiles / preWriteDurationS));
+      totalBlocks += addedNumBlockFiles;
+      long finalTotalBlocks = totalBlocks;
+      Thread scannerThread = new Thread(new Runnable() {
+        @Override
+        public void run() {
+          try {
+            long startScannerTime = System.currentTimeMillis();
+            scan(finalTotalBlocks, addedNumBlockFiles, addedNumBlockFiles, 0, addedNumBlockFiles, 0);
+            System.out.println("Scan time=" + ((System.currentTimeMillis() - startScannerTime) / 1000));
+          } catch (Exception e) {
+            e.printStackTrace();
+          }
+        }
+      });
+      scannerThread.start();
+
+      // The real workload
+      startWriteTime = System.currentTimeMillis();
+      for (int i = 0; i < numWriteOps; ++i) {
+        createBlockFile();
+      }
+      endWriteTime = System.currentTimeMillis();
+      double writeDurationS = (double) (endWriteTime - startWriteTime) / 1000;
+      System.out.println("Write Ops=" + numWriteOps
+              + ", time(s)=" + writeDurationS
+              + ", throughput=" + (numWriteOps / preWriteDurationS));
+
+      scannerThread.join();
+
+      scanner.shutdown();
+      assertFalse(scanner.getRunStatus());
+    } finally {
+      if (scanner != null) {
+        scanner.shutdown();
+        scanner = null;
+      }
+      cluster.shutdown();
+    }
+  }
+
   /**
    * Test that the timeslice throttle limits the report compiler thread's
    * execution time correctly. We test by scanning a large block pool and
